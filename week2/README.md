### Неделя 2
* [Материалы](https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie/home/week/2)

#### Заметки

1. **Метод ближайших соседей**. В регрессиях всяких, чем ближе объекты, тем ближе ответа, а в классификации, чем ближе объекты, тем скорее они принадлежат одному классу. В этом методе отсутствует этап обучения, т.к. всё, что мы делаем — это просто запоминаем выборку и при классификации объекта $x$ сортируем её:
$$ p(x,x^{(1)}) \leqslant p(x,x^{(2)}) \leqslant \ldots $$
Вот такой вот общий метрический алгоритм классфикации:
$$ a(x, X^l) = \arg\max_{y \in Y}{\sum_i^l{[y = y^{(i)}]w(i, x)}} $$
Ну ок. Если $w(i,x) = [i \leqslant k]$, то это будет метод $k$ ближайших соседей. Можно $k$ находить с помощью кросс-валидации (скользящего контроля) leave one out:
$$ LOO(k, X^l) = \sum_i^l[a(x_i, X^{l}\setminus x_i) \neq y_i] \rightarrow \min $$
Проблемы метода: `1)` Неоднозначность классификации при равном макс. числе объектов двух разных классов `2)` если последний, например, сосед очень далеко, то не дело, что он с тем же весом учавсвует, хорошо бы вес этот уменьшить для такого соседа.
2. **Метод Парзеновского окна**. Метрический алгоритм для которого:
$$ w(i,x) = K\big( \frac{\rho(x, x^{(i)})}{h} \big)$$
$K$ — ядро, невозрастающая функция, положительная на $[0, 1]$, $h$ — размер окна, дабы оно реагировало на плотность расположения объектов, берут: $h = \rho(x, x^{(k+1)})$. Правда тут тогда не понятно, как будут занулятся $w(i,x)$ при $i>k$, но это решаемо...
*Суть:* среди $k$ ближайших соседей мы смотрим на $k_i$ соседей класса $y^{(i)}$ и каждого из них взвешиваем, причём если объекты попали в окно ширины $h$, то вес у них положительный, но чем дальше объект от центра окна (т.е. от классифицируемого объекта $x$), тем меньше у него вес. Тогда, например, если мы смотрим на 5 ближайших соседей $x$ и серди них есть $2$ близких соседа класса $A$ и $3$ соседа класса $B$ которые довольно далеко от $x$, то не смотря на то, что соседей класса $B$ больше, максимум получится на классе $A$.**Метод потенциальных функций**. Ещё одно обощение метрического алгоритма классификации:$$ w(i,x) = \gamma^i K\big(\frac{\rho(x, x^{(i)})}{h_i} \big)$$
Аналогия: $h_i$ — радиус действия заряда в точке $x_i$, сила которого — $\gamma^i$, а класс выступает "знаком заряда".
В случае двух классов задача максимизации вырождается в линейных классификатор. И тогда $w(i,x)$ выступают признаками объекта $x$!
3. **Непараметрическая регрессия**. Параметрическая модель $f(\alpha, x)$ — это сложно, т.к. её ещё нужно придумать. Поэтому давайте вместо задачи линейной регрессии:
$$Q(\alpha, X^l) = \sum_{i=1}^l{w_i(f(\alpha, x^{(i)}) - y^{(i)})^2} \rightarrow_{\alpha} \min$$
(тут $w_i$ — это значимость $i$-го объекта), решать такую задачу:
$$Q(\alpha, X^l) = \sum_{i=1}^l{w_i(x)(\alpha - y^{(i)})^2} \rightarrow_{\alpha} \min$$
Тут у нас функция $f$ приближается константой $\alpha$ в окрестности объекта $x$ для которого мы выдаём ответ. Основная задача полагается на выбор $w_i(x)$ — это весовые функции, зависящие от рассотяния от $x$ до объектов обучающей выборки, как и в предыдущих пунктах определяемые (через ядро). Если подифференцировать и приравнять к нулю, т.е. решить задачу оптимизации, то полчится формула *Надарая-Ватсона!* (ядерное сглаживание):
$$ a_h(x; X^l) = \frac{\sum_{i=1}^l{y_iw_i(x)}}{\sum_{i=1}^l{w_i(x)}},\ w_i(x) = K\big( \frac{\rho(x, x^{(i)})}{h} \big)$$ 
Выбор ядра не так важен, в отличие от выбора размера окна, ибо ядро по сути влияет лишь на свойство гладкости, что не всегда важно.
**Проблема:** выброс в значении $y_i$ сильно искажает оценку ядерного сглаживания. Чтобы с этим бороться используют дополнительное взвешивание, вычисляя невязки: $\epsilon_i = |a_h(x; X^l\setminus x_i) - y_i|$. Чем невзяка больше, тем меньше итоговый вес $x_i$. Для более полного рассказа смотри алгоритм *LOWESS* (LOcally WEighted Scatter plot Smoothing).